from playwright.sync_api import sync_playwright

URL = "https://www.jobkorea.co.kr/Search/?stext=í”„ë¡ íŠ¸ì—”ë“œ"

# https://www.jobkorea.co.kr/Search?stext=í”„ë¡ íŠ¸ì—”ë“œ&Page_No=4
results = []

def get_pages(url):
  with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    context = browser.new_context(
        user_agent=(
           "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    )

    page = context.new_page()
    page.goto(url)

    # í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    page.wait_for_selector('nav[aria-label="pagination"]')
    pagenation = page.locator('nav[aria-label="pagination"]')
    total_pages = pagenation.locator('li:not(:first-child):not(:last-child)').count()

    browser.close()
    return total_pages

def scrape_jobkorea(url):
  print(f"ğŸ” Scraping {url}...")

  with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    context = browser.new_context(
        user_agent=(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    )

    page = context.new_page()
    page.goto(url)

    # 1ï¸âƒ£ ì²« ì¹´ë“œ ë¡œë”© ëŒ€ê¸°
    page.wait_for_selector('div[data-sentry-component="CardJob"]')

    # 2ï¸âƒ£ ìŠ¤í¬ë¡¤í•´ì„œ ì¹´ë“œ ë” ë¡œë”©
    for _ in range(5):
      page.mouse.wheel(0, 3000)
      page.wait_for_timeout(1500)

    # 3ï¸âƒ£ ì¹´ë“œ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
    cards = page.locator('div[data-sentry-component="CardJob"]')
    print("ğŸƒ ì¹´ë“œ ê°œìˆ˜:", cards.count())

    # 4ï¸âƒ£ ì¹´ë“œ ì •ë³´ ì¶”ì¶œ
    for i in range(cards.count()):
      card = cards.nth(i)

      excluded_colors = ["yellow", "theme-primary", "pink", "theme-secondary4", "theme-secondary2", "theme-secondary3"]

      # excluded_colors ë¦¬ìŠ¤íŠ¸ì˜ í•­ëª©ë“¤ì„ ê°ê° :not([data-accent-color="..."]) ìœ¼ë¡œ ë§Œë“¤ì–´
      # í•˜ë‚˜ì˜ ì„ íƒì ë¬¸ìì—´ë¡œ ê²°í•©í•œ ë’¤ locatorì— ì ìš©í•©ë‹ˆë‹¤.
      not_selectors = ''.join([f':not([data-accent-color="{c}"])' for c in excluded_colors])
      selector = f'span[data-sentry-element="Typography"]{not_selectors}'
      texts = card.locator(selector).all_inner_texts()

      title = texts[0]
      company = texts[1]
      location = texts[3]

      if not title:
        print(f"{i + 1} ë²ˆì§¸ ì¹´ë“œì—ì„œ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("texts:", texts)
      elif not company:
        print(f"{i + 1} ë²ˆì§¸ ì¹´ë“œì—ì„œ íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("texts:", texts)
      elif not location:
        print(f"{i + 1} ë²ˆì§¸ ì¹´ë“œì—ì„œ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("texts:", texts)
      else:
        continue

      link = card.locator("a", has_text=title).get_attribute('href')

      # results.append({
      #   "title": title.strip(),
      #   "company": company.strip(),
      #   "location": location.strip(),
      #   "url": link.strip()
      # })

    browser.close()

# ê²°ê³¼ í™•ì¸
total_pages = get_pages(URL)

for page in range(total_pages):
  scrape_jobkorea(f"{URL}&Page_No={page + 1}")

print("ì´ ê²°ê³¼ ê°œìˆ˜:", len(results))

for r in results[:5]:
  print(r)

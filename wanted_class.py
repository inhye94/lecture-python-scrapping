# ì›í‹°ë“œ í™ˆë¶€í„° ì‹œì‘
# playwrightë¡œ ê²€ìƒ‰ > í¬ì§€ì…˜ > 3ì´ˆ ë©ˆì¶¤ > ìŠ¤í¬ë¡¤ 3ë²ˆ ë‚´ë¦° ë’¤ > ì¹´ë“œ ê¸ì–´ì˜¤ê¸°

from playwright.sync_api import sync_playwright
import time
from bs4 import BeautifulSoup
import csv


class WantedScraper:
  BASE_URL = "https://www.wanted.co.kr"

  def __init__(self):
    self.browser = None
    self.content = None
    self.page = None

  # ë¸Œë¼ìš°ì € ì„¤ì •
  def open(self):
    self.playwright = sync_playwright().start()
    self.browser = self.playwright.chromium.launch(headless=True)
    context = self.browser.new_context(
      user_agent=(
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
          "AppleWebKit/537.36 (KHTML, like Gecko) "
          "Chrome/120.0.0.0 Safari/537.36"
        )
      )

    self.page = context.new_page()
  
  def close(self):
    if (self.browser):
      self.browser.close()
    self.playwright.stop()

  # ê´‘ê³  ë‹«ê¸°
  def close_ad(self):
    try:
      ad_iframe = self.page.wait_for_selector('iframe[title="WANTED"]', timeout=5000)
      close_button = ad_iframe.locator('div.close').first
      close_button.wait_for(timeout=3000)
      close_button.click()
    except:
      pass

  # ê²€ìƒ‰
  def search(self, keyword):
    self.page.goto(f"{self.BASE_URL}/search?query={keyword}&tab=position")
    
    # self.page.goto(self.BASE_URL)
    # self.close_ad()

    # self.page.wait_for_selector('button[aria-label="ê²€ìƒ‰"]')

    # self.page.locator('button[aria-label="ê²€ìƒ‰"]').click()
    # self.page.get_by_placeholder("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.").fill(keyword)
    # self.page.keyboard.press("Enter")


    # time.sleep(3)
    # self.page.click('a#search_tab_position')
    # time.sleep(3)

  # ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°
  def scroll_down(self, times=3):
    for _ in range(times):
      # page.mouse.wheel(0, 300)
      self.page.keyboard.press("End")
      time.sleep(2)
  
  # ì¹´ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
  def parse_jobs(self):
    # ì¹´ë“œ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
    self.content = self.page.content()
    soup = BeautifulSoup(self.content, "html.parser")
    cards = soup.find_all("a", {"data-position-list-type": "card"})

    jobs = []

    for card in cards:
      link = f"{self.BASE_URL}{card['href']}"
      company = card["data-company-name"]

      title_element = card.find('strong')
      title = title_element.text if title_element else None

      reward_element = card.find('span', class_="JobCard_reward__oCSIQ")
      reward = reward_element.text if reward_element else None

      jobs.append({
        "title": title,
        "company": company,
        "link": link,
        "reward": reward
      })

    return jobs
  

  # ì›í‹°ë“œ ì±„ìš© ì •ë³´ ìŠ¤í¬ë˜í•‘
  def scrape(self, keyword):
    print(f"ğŸ” Scraping wanted.co.kr for {keyword}...")
    self.search(keyword)
    self.scroll_down(times=3)
    return self.parse_jobs()

  # CSVë¡œ ì €ì¥
  @staticmethod
  def save_to_csv(datas, filename):
    file = open(f"scraped_datas/wanted_{filename}_jobs.csv", "w", encoding="utf-8")
    writer = csv.DictWriter(file, fieldnames=datas[0].keys())
    writer.writeheader()
    writer.writerows(datas)

# í‚¤ì›Œë“œë¡œ ìŠ¤í¬ë˜í•‘ ë ˆì¸ ê³ 
keywords = ["react", "nextjs", "flutter"]
scraper = WantedScraper()
scraper.open()

for keyword in keywords:
  datas = scraper.scrape(keyword)
  scraper.save_to_csv(datas, keyword)

scraper.close()